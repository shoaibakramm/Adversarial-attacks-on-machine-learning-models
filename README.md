# Adversarial-attacks-on-machine-learning-models

In this repo i replicate adversarial attacks and test them on machine learning models. I use the FGSM(fast gradient sign method) and the gaussian noise method for the pertubation of the inputs to achieve the goal. Additionally, i have also implemented the prompt injection adversarial attack on large language models using the Universal prompt injection.


Gaussian (random) noise attack.
FGSM (gradient-based) attack.
